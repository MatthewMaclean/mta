{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re, os, sys\n",
    "#sys.path.insert(0, os.path.realpath('/content/jupyter/mta-accessibility/notebooks/routing'))\n",
    "sys.path.insert(0, os.getcwd())\n",
    "from gcs_utils import gcs_util\n",
    "import GTFS_Utils as gu\n",
    "\n",
    "import os.path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from google.transit import gtfs_realtime_pb2 as gtfs_rt\n",
    "from protobuf_to_dict import protobuf_to_dict\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from itertools import groupby\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "pd.set_option('max_colwidth', 200)\n",
    "gcs = gcs_util()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class Plog:    \n",
    "    def __init__(self):        \n",
    "        log = logging.getLogger('gtfs_consolidator')\n",
    "        log.setLevel(logging.DEBUG)\n",
    "\n",
    "        filename = \"consolidator{}.log\".format(datetime.now().strftime(\"%Y%m%d-%I%M%S\"))\n",
    "        # create file handler which logs even debug messages\n",
    "        fh = logging.FileHandler(filename)\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create console handler with a higher log level\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.ERROR)\n",
    "\n",
    "        # create formatter and add it to the handlers\n",
    "        formatter = logging.Formatter('%(asctime)s-%(levelname)s: %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        ch.setFormatter(formatter)\n",
    "\n",
    "        if log.hasHandlers():\n",
    "            log.handlers.clear()\n",
    "        # add the handlers to the logger\n",
    "        log.addHandler(fh)\n",
    "        log.addHandler(ch)\n",
    "\n",
    "        self.log_instance = log\n",
    "                    \n",
    "    def error(self, *args, **kwargs):\n",
    "        self.log_instance.error(*args, **kwargs)\n",
    "    def info(self, *args, **kwargs):\n",
    "        self.log_instance.info(*args, **kwargs)\n",
    "    def warn(self, *args, **kwargs):\n",
    "        self.log_instance.warning(*args, **kwargs)\n",
    "        \n",
    "log = Plog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTFS Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vehicle_columns = ['alert.header_text.translation',\n",
    "       'current_status', 'current_stop_sequence', 'id', \n",
    " 'route_id','start_date', 'start_time', 'stop_id', 'stop_name', 'timestamp','trip_id']\n",
    "\n",
    "trip_columns = ['alert_header_text_translation',\n",
    "       'arrival_time', 'departure_time', 'id', 'route_id', 'start_date',\n",
    "       'start_time', 'stop_id', 'stop_name', 'trip_id']\n",
    "\n",
    "def validate_hourly(df: pd.DataFrame, msg_type: str, date: str, hour: str):\n",
    "    \"\"\"Sanity check GTFS files rolled up hourly\n",
    "        Checks that the column schema is as expected\n",
    "        Checks that there are timestamps spanning at least the full hour\n",
    "        and that there are no large gaps in time\n",
    "    \"\"\"\n",
    "    # Max gap in time between rows.\n",
    "    MAX_GAP_THRESHOLD = pd.to_timedelta(15, unit='m')\n",
    "    \n",
    "    error_msg = []\n",
    "    \n",
    "    df_columns = list(df.columns)    \n",
    "    # Verify column schema\n",
    "    # Expected columns schema should at least be a subset of actual schema\n",
    "    if msg_type == 'vehicle_updates':\n",
    "        if not (all(x in df_columns for x in vehicle_columns)):\n",
    "            error_msg.append(\"Vehicle schema incorrect\")\n",
    "    elif msg_type == 'trip_updates':\n",
    "        if not (all(x in df_columns for x in vehicle_columns)):\n",
    "            error_msg.append(\"Trip update schema incorrect\")\n",
    "    else:\n",
    "        error_msg.append(\"Unrecognized msg type\")\n",
    "        \n",
    "    # Verify date range and diff\n",
    "    expected_beg = pd.Timestamp(f\"{date}-{hour}:00:00\")\n",
    "    expected_end = expected_beg + pd.to_timedelta(1, unit='h')\n",
    "    \n",
    "    beggining = df.timestamp.min()\n",
    "    end = df.timestamp.max()\n",
    "    max_diff = df.timestamp.sort_values().diff().max()\n",
    "        \n",
    "#     if not (expected_beg <= beggining and end <= expected_end):\n",
    "#         error_msg.append(f\"Expected [{expected_beg}, {expected_end}], actual [{beggining}, {end}]\")\n",
    "    \n",
    "    if (max_diff > MAX_GAP_THRESHOLD):\n",
    "        error_msg.append(f\"Maximum diff of {max_diff}, threshold {MAX_GAP_THRESHOLD}\")\n",
    "    \n",
    "    return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_blob_name(name: str):\n",
    "    \"\"\"Returns msg_type, date, time\"\"\"\n",
    "    pat = re.compile(\"realtime\\/(.*)_([0-9]*)-([0-9]*)\")\n",
    "    return pat.match(name).groups()\n",
    "\n",
    "\n",
    "blobs_by_date = {}\n",
    "for blob in gcs.list_blobs('realtime'):\n",
    "    \n",
    "    msg_type, date, time = parse_blob_name(blob.name)\n",
    "    blobs_by_date.setdefault(date, []).append(blob.name)    \n",
    "    \n",
    "for key,val in blobs_by_date.items():\n",
    "    print(f\"{key}: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200501: 2690\n",
      "20200502: 2708\n",
      "20200503: 2720\n",
      "20200504: 2710\n",
      "20200505: 2708\n",
      "20200506: 2720\n",
      "20200507: 2590\n",
      "20200512: 74\n",
      "20200419: 230\n",
      "20200420: 229\n",
      "20200422: 57\n",
      "20200423: 112\n",
      "20200424: 230\n",
      "20200425: 230\n",
      "20200426: 57\n",
      "20200427: 58\n",
      "20200428: 230\n",
      "20200429: 616\n",
      "20200430: 1073\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'03': 57, '07': 58, '08': 58, '09': 57}"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = \"20200419\"\n",
    "\n",
    "files = blobs_by_date[date]\n",
    "\n",
    "count = {}\n",
    "for f in files:\n",
    "    msg_type, date, time = parse_blob_name(f)\n",
    "    hour = time[:2]\n",
    "    c = count.setdefault(hour, 0)\n",
    "    count[hour] += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-13 14:24:25,453-ERROR: realtime/hourly/vehicle_updates_20200422_03 - ['Maximum diff of 0 days 23:36:17, threshold 0 days 00:15:00']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update done\n"
     ]
    }
   ],
   "source": [
    "bdf = pd.DataFrame()\n",
    "for date, file_lst in blobs_by_date.items():\n",
    "    veh_by_hour = {}\n",
    "    upd_by_hour = {}\n",
    "    \n",
    "    if date.startswith('202005'):\n",
    "        continue          \n",
    "    if date != '20200422':\n",
    "        continue\n",
    "        \n",
    "    # {'vehicle_updates', 'trip_updates'}\n",
    "    for file in file_lst:\n",
    "        msg_type, date, time = parse_blob_name(file)\n",
    "        hour = time[:2]\n",
    "        if msg_type == \"vehicle_updates\":\n",
    "            veh_by_hour.setdefault(hour, []).append(file)\n",
    "        elif msg_type == \"trip_updates\":\n",
    "            upd_by_hour.setdefault(hour, []).append(file)\n",
    "        else:\n",
    "            print(f\"Unknown type for file {file}\")\n",
    "                            \n",
    "    log.info(f\"{date} {len(file_lst)}\")\n",
    "    \n",
    "    def consolidate_files(file_lst: List[str]):        \n",
    "        df = pd.DataFrame()\n",
    "        for file in file_lst:\n",
    "            df = gcs.read_dataframe(file)\n",
    "            df = df.append(df, sort=True).drop_duplicates()\n",
    "        return df            \n",
    "    \n",
    "    def move_files_to_processed(files: List[str]):\n",
    "        for file in files:\n",
    "            file_name = file.split(\"/\")[1]\n",
    "            new_path = f\"realtime/processed_df/{file_name}\"            \n",
    "            gcs.move_blob(file, new_path)            \n",
    "    \n",
    "    for hour, files in veh_by_hour.items():        \n",
    "        msg_type = \"vehicle_updates\"        \n",
    "        df = consolidate_files(files)        \n",
    "        errors = validate_hourly(df, msg_type, date, hour)\n",
    "        output_name = f\"realtime/hourly/{msg_type}_{date}_{hour}\"\n",
    "        bdf = df\n",
    "        if len(errors):\n",
    "            log.error(f\"{output_name} - {errors}\")            \n",
    "        else:            \n",
    "            gcs.write_dataframe(df, output_name)\n",
    "            move_files_to_processed(files)\n",
    "            log.info(f\"Uploading {output_name}\")            \n",
    "    \n",
    "    for hour, files in upd_by_hour.items():\n",
    "        msg_type = \"trip_updates\"\n",
    "        df = consolidate_files(files)\n",
    "        # TODO Maybe validate these too?\n",
    "        output_name = f\"realtime/hourly/{msg_type}_{date}_{hour}\"\n",
    "        gcs.write_dataframe(df, output_name)\n",
    "        move_files_to_processed(files)\n",
    "        log.info(f\"Uploading {output_name}\")        \n",
    "                \n",
    "    \n",
    "print(\"Update done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-21 04:12:55 2020-04-22 04:57:30\n"
     ]
    }
   ],
   "source": [
    "print(bdf.timestamp.min(), bdf.timestamp.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert.header_text.translation</th>\n",
       "      <th>alert.informed_entity</th>\n",
       "      <th>current_status</th>\n",
       "      <th>current_stop_sequence</th>\n",
       "      <th>id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>trip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>000076</td>\n",
       "      <td>3</td>\n",
       "      <td>20200422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127N</td>\n",
       "      <td>Times Sq - 42 St</td>\n",
       "      <td>2020-04-21 04:12:55</td>\n",
       "      <td>146300_3..N42R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>000072</td>\n",
       "      <td>3</td>\n",
       "      <td>20200422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123N</td>\n",
       "      <td>72 St</td>\n",
       "      <td>2020-04-22 03:49:12</td>\n",
       "      <td>144200_3..N42R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>000118</td>\n",
       "      <td>6</td>\n",
       "      <td>20200422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640N</td>\n",
       "      <td>Brooklyn Bridge - City Hall</td>\n",
       "      <td>2020-04-22 03:53:51</td>\n",
       "      <td>144100_6..N01R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>000042</td>\n",
       "      <td>2</td>\n",
       "      <td>20200421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201N</td>\n",
       "      <td>Wakefield - 241 St</td>\n",
       "      <td>2020-04-22 03:56:34</td>\n",
       "      <td>135200_2..N01R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>000102</td>\n",
       "      <td>4</td>\n",
       "      <td>20200421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>635N</td>\n",
       "      <td>14 St - Union Sq</td>\n",
       "      <td>2020-04-22 03:57:48</td>\n",
       "      <td>139950_4..N01R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alert.header_text.translation alert.informed_entity  current_status  \\\n",
       "12                           NaN                   NaN             NaN   \n",
       "25                           NaN                   NaN             2.0   \n",
       "48                           NaN                   NaN             1.0   \n",
       "17                           NaN                   NaN             2.0   \n",
       "42                           NaN                   NaN             1.0   \n",
       "\n",
       "    current_stop_sequence      id route_id start_date start_time stop_id  \\\n",
       "12                    1.0  000076        3   20200422        NaN    127N   \n",
       "25                    2.0  000072        3   20200422        NaN    123N   \n",
       "48                    1.0  000118        6   20200422        NaN    640N   \n",
       "17                   49.0  000042        2   20200421        NaN    201N   \n",
       "42                   22.0  000102        4   20200421        NaN    635N   \n",
       "\n",
       "                      stop_name           timestamp         trip_id  \n",
       "12             Times Sq - 42 St 2020-04-21 04:12:55  146300_3..N42R  \n",
       "25                        72 St 2020-04-22 03:49:12  144200_3..N42R  \n",
       "48  Brooklyn Bridge - City Hall 2020-04-22 03:53:51  144100_6..N01R  \n",
       "17           Wakefield - 241 St 2020-04-22 03:56:34  135200_2..N01R  \n",
       "42             14 St - Union Sq 2020-04-22 03:57:48  139950_4..N01R  "
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf.sort_values('timestamp').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle_updates_20200417-173153\n",
      "vehicle_updates_20200417-173258\n",
      "vehicle_updates_20200417-173402\n"
     ]
    }
   ],
   "source": [
    "files = ['realtime/vehicle_updates_20200417-173153', 'realtime/vehicle_updates_20200417-173258', 'realtime/vehicle_updates_20200417-173402']\n",
    "for file in files:\n",
    "    file_name = file.split(\"/\")[1]\n",
    "    \n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split files between vehicle and trip updates\n",
    "vehicle_updates = []\n",
    "trip_updates = []\n",
    "\n",
    "for b in gcs.list_blobs('realtime'):\n",
    "    if \"trip_updates\" in b.name:\n",
    "        trip_updates.append(b)\n",
    "    elif \"vehicle_updates\" in b.name:\n",
    "        vehicle_updates.append(b)    \n",
    "    else:\n",
    "        print(f\"Found unknown path {b.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20200417': ['realtime/vehicle_updates_20200417-173153',\n",
       "  'realtime/vehicle_updates_20200417-173258',\n",
       "  'realtime/vehicle_updates_20200417-173402',\n",
       "  'realtime/vehicle_updates_20200417-173508',\n",
       "  'realtime/vehicle_updates_20200417-173611',\n",
       "  'realtime/vehicle_updates_20200417-173715',\n",
       "  'realtime/vehicle_updates_20200417-173819',\n",
       "  'realtime/vehicle_updates_20200417-173923',\n",
       "  'realtime/vehicle_updates_20200417-174027',\n",
       "  'realtime/vehicle_updates_20200417-174130']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_dates = set()\n",
    "files_by_dates = {}\n",
    "\n",
    "for blob in vehicle_updates[:10]:\n",
    "    name = blob.name\n",
    "    date = name.split(\"-\")[0].split(\"_\")[2]\n",
    "    \n",
    "    file_lst = files_by_dates.get(date, [])\n",
    "    file_lst.append(blob.name)      \n",
    "    files_by_dates[date] = file_lst\n",
    "    \n",
    "files_by_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74259\n"
     ]
    }
   ],
   "source": [
    "# Consolidate vehicle updates\n",
    "unique_dates = set()\n",
    "files_by_dates = {}\n",
    "\n",
    "for blob in vehicle_updates:\n",
    "    name = blob.name\n",
    "    date = name.split(\"-\")[0].split(\"_\")[2]\n",
    "    \n",
    "    file_lst = files_by_dates.get(date, [])\n",
    "    file_lst.append(blob.name)\n",
    "      \n",
    "    files_by_dates[date] = file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[numpy.datetime64('NaT'),\n",
       " numpy.datetime64('2020-04-17T17:24:35.000000000'),\n",
       " numpy.datetime64('2020-04-17T17:26:17.000000000'),\n",
       " numpy.datetime64('2020-04-17T17:27:33.000000000'),\n",
       " numpy.datetime64('2020-04-17T17:28:21.000000000'),\n",
       " numpy.datetime64('2020-04-17T17:29:10.000000000'),\n",
       " numpy.datetime64('2020-04-17T17:29:21.000000000'),\n",
       " numpy.datetime64('2020-04-17T17:30:13.000000000'),\n",
       " numpy.datetime64('2020-04-17T17:30:15.000000000'),\n",
       " numpy.datetime64('2020-04-17T17:30:19.000000000')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = df.timestamp.unique()\n",
    "l.sort()\n",
    "\n",
    "list(l[:10])"
   ]
  }
 ],
 "metadata": {
  "alphastudio": {
   "as_jupyter_image_name": "gcr.io/ts-quantsource/as-jupyter-v6",
   "as_jupyter_image_tag": "f5cc425e18e2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
